{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec2cdd5",
   "metadata": {},
   "source": [
    "# 自然言語処理入門\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shinchu/dataviz-notebooks/blob/main/week_5/intro-to-nlp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f85a0",
   "metadata": {},
   "source": [
    "テキストデータの処理と分析の基礎である自然言語処理を概観しましょう。\n",
    "\n",
    "まず、テキストデータの前処理を段階を踏んで見ていきます。\n",
    "\n",
    "次に、簡単な例から自然言語処理の考え方を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2e058",
   "metadata": {},
   "source": [
    "## テキストデータの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238e909",
   "metadata": {},
   "source": [
    "テキストデータを分析する際には、基本的に以下の処理が行われます。\n",
    "\n",
    "1. 分かち書き（形態素解析）\n",
    "2. 品詞付与\n",
    "3. 係り受け解析\n",
    "4. 固有表現抽出\n",
    "5. 原形抽出\n",
    "\n",
    "\n",
    "形態素は、「言葉が意味を持つまとまりの単語の最小単位」で、形態素解析は、文章を一つ一つの形態素に分ける技術です。単語が区切られていない日本語などの言語では特に重要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5c429",
   "metadata": {},
   "source": [
    "日本語を例に、`spaCy`と`GiNZA`というライブラリを使って処理の過程を見ていきましょう。\n",
    "\n",
    "`spaCy`では上の一連の処理をまとめて行ってくれます。\n",
    "\n",
    "テキストデータとして、オー・ヘンリー（結城浩訳）[『最後の一枚の葉』](https://www.hyuki.com/trans/leaf.html)の冒頭部分を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e459e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "ワシントン・スクエア西にある小地区は、 道路が狂ったように入り組んでおり、 「プレース」と呼ばれる区域に小さく分かれておりました。 この「プレース」は不可思議な角度と曲線を描いており、 一、二回自分自身と交差している通りがあるほどでした。 かつて、ある画家は、この通りが貴重な可能性を持っていることを発見しました。 例えば絵や紙やキャンバスの請求書を手にした取り立て屋を考えてみてください。 取り立て屋は、この道を歩き回ったあげく、 ぐるりと元のところまで戻ってくるに違いありません。 一セントも取り立てることができずにね。\n",
    "それで、芸術家たちはまもなく、奇妙で古いグリニッチ・ヴィレッジへとやってきました。 そして、北向きの窓と十八世紀の切り妻とオランダ風の屋根裏部屋と安い賃貸料を探してうろついたのです。 やがて、彼らは しろめ製のマグやこんろ付き卓上なべを一、二個、六番街から持ち込み、 「コロニー」を形成することになりました。\n",
    "ずんぐりした三階建ての煉瓦造りの最上階では、スーとジョンジーがアトリエを持っていました。 「ジョンジー」はジョアンナの愛称です。 スーはメイン州の、ジョンジーはカリフォルニア州の出身でした。 二人は八番街の「デルモニコの店」の定食で出会い、 芸術と、チコリーのサラダと、ビショップ・スリーブの趣味がぴったりだとわかって、 共同のアトリエを持つことになったのでした。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54829e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインストール\n",
    "\n",
    "!pip install spacy ginza ja-ginza\n",
    "!pip install sklearn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import spacy\n",
    "\n",
    "# 日本語モデルのロード\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "\n",
    "# 解析\n",
    "doc = nlp(text)\n",
    "\n",
    "# 結果の確認\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86492713",
   "metadata": {},
   "source": [
    "形態素解析の結果には、語の原形や品詞の情報も含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818bd3e7",
   "metadata": {},
   "source": [
    "係り受け（単語の修飾関係）は、次のように確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 解析結果をpandasのDataFrameに入れる\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"text\": token.text,\n",
    "    \"lemma_\": token.lemma_,\n",
    "    \"pos_\": token.pos_,\n",
    "    \"tag_\": token.tag_,\n",
    "    \"dep_\": token.dep_,\n",
    "    \"children\": list(token.children)\n",
    "} for token in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839896a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d99751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 係り受けの図を表示する\n",
    "\n",
    "spacy.displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db53efd",
   "metadata": {},
   "source": [
    "それでは、単語の使用頻度を数えてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88206e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in doc)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135cab1",
   "metadata": {},
   "source": [
    "句読点や助詞など、意味がなさそうな言葉ばかりです。\n",
    "\n",
    "より意味がある語を取り出すために、分析対象とする品詞を指定しましょう。具体的には、内容語である名詞、動詞、形容詞（、固有名詞）を指定すればよいでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析対象とする品詞の指定\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")\n",
    "\n",
    "# 再度単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in doc if token.pos_ in include_pos)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897620e",
   "metadata": {},
   "source": [
    "ちょっとよくなりました。でも、「こと」「ある」「いる」などの一般的な名詞や動詞が多いように思えます。\n",
    "\n",
    "これらを不要語として指定し、除去しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b29334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析対象とする品詞と不要語（ストップワード）を指定する\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")\n",
    "stopwords = (\"する\", \"ある\", \"おる\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\")\n",
    "\n",
    "# 再度単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in doc\n",
    "                  if token.pos_ in include_pos and token.lemma_ not in stopwords)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84de40a",
   "metadata": {},
   "source": [
    "ずっと良くなりました。これだけの作業で、Bag-of-Wordsを作成することができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a763c12",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066d40e",
   "metadata": {},
   "source": [
    "## 自然言語処理とは"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88193a7f",
   "metadata": {},
   "source": [
    "日本語や英語など、私たちが普段使っている言葉を自然言語（Natural Language）と言います。自然言語処理（Natural Language Processing）とは、自然言語を処理する分野です。\n",
    "\n",
    "自然言語処理の目標は、人の話す言葉をコンピュータに理解させ、私たちにとって役に立つことをコンピュータに行わせることです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82428cfe",
   "metadata": {},
   "source": [
    "私たちの言葉は「文字」によって表現することができます。そして、言葉の意味は「単語」（正確には形態素）によって構成されます。そのため、自然言語をコンピュータに理解させるためには、「単語の意味」を理解させることが重要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d4c5d",
   "metadata": {},
   "source": [
    "ここでは、統計情報から単語の意味を表現する手法を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad6247",
   "metadata": {},
   "source": [
    "## 統計的な手法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfaab9",
   "metadata": {},
   "source": [
    "自然言語処理の研究や応用のために目的をもって収集されたテキストデータを「コーパス」と呼びます。WikipediaやGoogle Newsなどのテキストデータや、シェイクスピアや夏目漱石などの作品群もコーパスです。\n",
    "\n",
    "コーパスはテキストデータであり、そこに含まれる文章は人によって書かれたものです。これはつまり、コーパスには自然言語に対する人の知識が含まれているということです。文章の書き方、単語の選び方、単語の意味などがコーパスには含まれています。\n",
    "\n",
    "カウントベースの手法の目標は、人の知識が詰まったコーパスから、自動的に、効率よく、そのエッセンスを抽出することです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dd205",
   "metadata": {},
   "source": [
    "## 簡単なコーパスの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75ab17",
   "metadata": {},
   "source": [
    "テキストデータを単語に分割し、分割した単語を単語IDのリストへ変換することで、データの前処理を行いましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527deca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小文字に変換\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a638cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句点の前にスペースを挿入\n",
    "text = text.replace(\".\", \" .\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e857600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文を単語に分割する\n",
    "words = text.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3eb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語のIDと単語の対応表を作る\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29090d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d2a13",
   "metadata": {},
   "source": [
    "この2つの辞書を使えば、単語から単語IDの検索と、単語IDから単語の検索ができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd884",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id[\"i\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf38f4",
   "metadata": {},
   "source": [
    "最後に、単語のリストを単語IDのリストに変換し、NumPy配列に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed646a",
   "metadata": {},
   "source": [
    "これでコーパスの前処理は終了です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd7f53",
   "metadata": {},
   "source": [
    "## 単語の分散表現\n",
    "\n",
    "\n",
    "次に、コーパスを使って単語の意味を抽出しましょう。具体的には、単語をベクトルで表すことを目指します。これは、自然言語処理の分野では、単語の分散表現と呼ばれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41437b8d",
   "metadata": {},
   "source": [
    "単語の分散表現に関する手法は、「単語の意味は、周囲の単語によって形成される」というアイデアに基づいています。これは、分布仮説と呼ばれるものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14e5f7",
   "metadata": {},
   "source": [
    "分布仮説では、単語自体には意味がなく、その単語の「コンテキスト（文脈）」によって、単語の意味が形成されると言われています。\n",
    "\n",
    "たしかに、意味的に同じ単語は、同じような文脈で多く出現します。\n",
    "\n",
    "例えば、\n",
    "\n",
    "* I drink beer.\n",
    "* We drink wine.\n",
    "\n",
    "のようにdrinkの近くには飲み物があらわれやすいでしょう。\n",
    "\n",
    "\n",
    "* I guzzle beer.\n",
    "* We guzzle wine.\n",
    "\n",
    "のような文章では、guzzleという単語がdrinkと同じような文脈で使われていることが分かります。そして、guzzleとdrinkが近い意味の単語だということが導けます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e818d28",
   "metadata": {},
   "source": [
    "## 共起行列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebded83",
   "metadata": {},
   "source": [
    "分布仮説に基づいて、単語をベクトルで表す方法を考えます。\n",
    "\n",
    "素直な方法は、周囲の単語を数えることです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc101a2",
   "metadata": {},
   "source": [
    "さきほど用意したコーパスに含まれるそれぞれの単語について、そのコンテキスト（目当ての単語の周囲）に含まれる単語の頻度を数えていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fefe2cd",
   "metadata": {},
   "source": [
    "例えば、youという単語に着目すると、\n",
    "\n",
    "||you|say|goodbye|and|i|hello|.|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|you|0|1|0|0|0|0|0|\n",
    "\n",
    "このような表現になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092ce7b",
   "metadata": {},
   "source": [
    "全ての語に対してこれを数えると、\n",
    "\n",
    "||you|say|goodbye|and|i|hello|.|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|you|0|1|0|0|0|0|0|\n",
    "|say|1|0|1|0|1|1|0|\n",
    "|goodbye|0|1|0|1|0|0|0|\n",
    "|and|0|0|1|0|1|0|0|\n",
    "|i|0|1|0|1|0|0|0|\n",
    "|hello|0|1|0|0|0|0|1|\n",
    "|.|0|0|0|0|0|1|0|\n",
    "\n",
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f10b25",
   "metadata": {},
   "source": [
    "これをNumPy配列にすることで、共起行列ができます。この共起行列を使うと、各単語の分散表現が求められます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fc90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# youの分散表現\n",
    "print(id_to_word[0], C[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# andの分散表現\n",
    "print(id_to_word[3], C[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304dac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "        \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb619b8",
   "metadata": {},
   "source": [
    "単語の分散表現を使うことで、単語の類似度を計算するなど、より高度な処理ができるようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d3064",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcef068",
   "metadata": {},
   "source": [
    "テキストデータの可視化には主に単語の出現頻度を使いました。これは、多く出現する単語ほど重要である、という直観的な考えに基づく指標です。\n",
    "\n",
    "しかし、一方で、いろいろな文書に多く出現すると予測される「わたし」などの言葉は、出現頻度のわりにさほど重要ではないと考えられます。このような言葉を不要語として指定し、分析から除外することもできますが、この点を考慮した単語の重要度の指標として、tf-idfがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a95a4c",
   "metadata": {},
   "source": [
    "tf-idfは、term frequency（単語頻度）-inverse document frequency（逆文書頻度）の略です。具体的には、「ある文書内である単語がどれくらい多い頻度で出現するか」を表すterm frequencyと、「全文書内である単語を含む文書がどれくらい少ない頻度で出現するか」を表すinverse document frequencyをかけ合わせた値です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0f9cf",
   "metadata": {},
   "source": [
    "簡単に言うと、どの文書にもよく出てくる単語の重要度を下げて、あまり出てこない単語の重要度を上げるための工夫です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4fef2",
   "metadata": {},
   "source": [
    "tf-idfはヒューリスティックな指標で、理論的な根拠はあまりありませんが、様々な場面でうまく重要語を抽出でき、文書を特徴づけることができることが経験的に知られています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77658a17",
   "metadata": {},
   "source": [
    "tf-idfについてもっと詳しく見る：\n",
    "\n",
    "[https://atmarkit.itmedia.co.jp/ait/articles/2112/23/news028.html](https://atmarkit.itmedia.co.jp/ait/articles/2112/23/news028.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
