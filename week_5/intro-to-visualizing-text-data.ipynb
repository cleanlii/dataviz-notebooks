{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523005b4",
   "metadata": {},
   "source": [
    "# テキストデータの可視化\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shinchu/dataviz-notebooks/blob/main/week_5/intro-to-visualizing-text-data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748ec8f",
   "metadata": {},
   "source": [
    "今回は、[青空文庫](https://www.aozora.gr.jp/)に収録されている、夏目漱石の[『三四郎』](https://www.aozora.gr.jp/cards/000148/card794.html)という作品を使って、テキストデータの可視化を練習します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb744af",
   "metadata": {},
   "source": [
    "## 基礎的な解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944becfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ginza==4.0.6 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (4.0.6)\n",
      "Requirement already satisfied: ja-ginza==4.0.0 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: SudachiDict-core>=20200330 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from ginza==4.0.6) (20221021)\n",
      "Requirement already satisfied: spacy<3.0.0,>=2.3.2 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from ginza==4.0.6) (2.3.8)\n",
      "Requirement already satisfied: SudachiPy>=0.4.9 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from ginza==4.0.6) (0.6.6)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (1.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (0.7.9)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (7.4.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (2.28.1)\n",
      "Requirement already satisfied: setuptools in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (65.3.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (1.23.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (3.0.8)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from spacy<3.0.0,>=2.3.2->ginza==4.0.6) (1.0.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza==4.0.6) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza==4.0.6) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza==4.0.6) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->ginza==4.0.6) (3.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sklearn in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (0.0.post1)\n",
      "Requirement already satisfied: pandas in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from pandas) (1.23.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shu/.virtualenvs/datatools/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ライブラリのインストール\n",
    "\n",
    "!pip install ginza==4.0.6 ja-ginza==4.0.0\n",
    "!pip install sklearn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0bbf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-09 15:16:51--  https://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip\n",
      "Resolving proxy.noc.titech.ac.jp (proxy.noc.titech.ac.jp)... 131.112.125.238\n",
      "Connecting to proxy.noc.titech.ac.jp (proxy.noc.titech.ac.jp)|131.112.125.238|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 158711 (155K) [application/zip]\n",
      "Saving to: ‘794_ruby_4237.zip’\n",
      "\n",
      "794_ruby_4237.zip   100%[===================>] 154.99K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2022-11-09 15:16:51 (14.1 MB/s) - ‘794_ruby_4237.zip’ saved [158711/158711]\n",
      "\n",
      "Archive:  794_ruby_4237.zip\n",
      "  inflating: text/sanshiro.txt       \n"
     ]
    }
   ],
   "source": [
    "# ファイルをダウンロードする\n",
    "!wget https://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip\n",
    "# textフォルダ作る\n",
    "!mkdir -p text\n",
    "# ファイルをtextフォルダに解凍\n",
    "!unzip -d text -o 794_ruby_4237.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbda573",
   "metadata": {},
   "source": [
    "正規表現で青空文庫のルビ、注、アクセントの記号を取り除きます。\n",
    "\n",
    "同時に、文字コードをShift-JISからUTF-8にします。\n",
    "\n",
    "```\n",
    "\n",
    "-------------------------------------------------------\n",
    "【テキスト中に現れる記号について】\n",
    "\n",
    "《》：ルビ\n",
    "（例）頓狂《とんきょう》\n",
    "\n",
    "｜：ルビの付く文字列の始まりを特定する記号\n",
    "（例）福岡県｜京都郡《みやこぐん》\n",
    "\n",
    "［＃］：入力者注　主に外字の説明や、傍点の位置の指定\n",
    "　　　（数字は、JIS X 0213の面区点番号またはUnicode、底本のページと行数）\n",
    "（例）※［＃「魚＋師のつくり」、第4水準2-93-37］\n",
    "\n",
    "〔〕：アクセント分解された欧文をかこむ\n",
    "（例）〔ve'rite'《ヴェリテ》 vraie《ヴレイ》.〕\n",
    "アクセント分解についての詳細は下記URLを参照してください\n",
    "http://www.aozora.gr.jp/accent_separation.html\n",
    "-------------------------------------------------------\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b90a3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "input_fn = \"text/sanshiro.txt\"\n",
    "output_fn = \"text/sanshiro.stripruby.txt\"\n",
    "\n",
    "with open(input_fn, encoding=\"shift_jis\") as fin, open(output_fn, mode=\"w\") as fout:\n",
    "    for line in fin:\n",
    "        fout.write(re.sub(r\"《[^》]+》|［[^］]+］|〔[^〕]+〕| [｜]\", \"\", line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063709e3",
   "metadata": {},
   "source": [
    "冒頭と末尾の説明を取り除きます（何行取り除くかは目視で確認）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f524b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: illegal line count -- -14\r\n",
      "tail: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n +22 text/sanshiro.stripruby.txt | head -n -14 > text/sanshiro.corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40cb7e0",
   "metadata": {},
   "source": [
    "これで、テキストファイルを扱う準備ができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a55698",
   "metadata": {},
   "source": [
    "### 形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e74bcb",
   "metadata": {},
   "source": [
    "形態素解析は、文章を一つ一つの形態素に分ける技術です。形態素は、「言葉が意味を持つまとまりの単語の最小単位」です。\n",
    "\n",
    "spaCyで日本語の形態素解析モデル（`ja_ginza`）をロードして、形態素解析をしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0928a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "これ\n",
      "は\n",
      "文章\n",
      "です\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "text = \"これは文章です。\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf136a",
   "metadata": {},
   "source": [
    "形態素解析の結果には、語の原形や品詞の情報も含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c7d3e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昨日\t昨日\tNOUN\t名詞-普通名詞-副詞可能\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "天気\t天気\tNOUN\t名詞-普通名詞-一般\n",
      "は\tは\tADP\t助詞-係助詞\n",
      "晴れ\t晴れ\tNOUN\t名詞-普通名詞-一般\n",
      "でし\tです\tAUX\t助動詞\n",
      "た\tた\tAUX\t助動詞\n",
      "。\t。\tPUNCT\t補助記号-句点\n",
      "明日\t明日\tNOUN\t名詞-普通名詞-副詞可能\n",
      "も\tも\tADP\t助詞-係助詞\n",
      "晴れる\t晴れる\tVERB\t動詞-一般\n",
      "でしょう\tです\tAUX\t助動詞\n",
      "。\t。\tPUNCT\t補助記号-句点\n"
     ]
    }
   ],
   "source": [
    "text = \"昨日の天気は晴れでした。明日も晴れるでしょう。\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962cb803",
   "metadata": {},
   "source": [
    "それでは、『三四郎』に出現する単語の頻度を数えてみましょう。\n",
    "\n",
    "テキストファイルを読み込んで形態素解析を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2eba820",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = \"text/sanshiro.corpus.txt\"\n",
    "output_fn = \"text/sanshiro.wakati.txt\"\n",
    "\n",
    "with open(input_fn, \"r\") as fin, open(output_fn, \"w\") as fout:\n",
    "    for line in fin:\n",
    "        tokens = [token.text for token in nlp(line.rstrip())]\n",
    "        fout.write(' '.join(tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256408c",
   "metadata": {},
   "source": [
    "出力されたファイルを確認すると、分かち書きされていることが分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7240a35",
   "metadata": {},
   "source": [
    "次に、使用頻度の高い単語を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80b6f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  614 言う\n",
      "  385 女\n",
      "  373 さん\n",
      "  314 先生\n",
      "  297 出る\n",
      "  270 見る\n",
      "  262 聞く\n",
      "  262 人\n",
      "  259 時\n",
      "  247 思う\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 分析対象とする品詞（内容語 - 名詞、動詞、形容詞）と不要語（ストップワード）を指定する\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\")\n",
    "\n",
    "# ファイルの読み込み、テキストを一行ずつ解析\n",
    "all_tokens = []\n",
    "with open(input_fn, \"r\") as f:\n",
    "    for line in f:\n",
    "        tokens = [token for token in nlp(line)]\n",
    "        all_tokens.extend(tokens)\n",
    "        \n",
    "# 単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in all_tokens if token.pos_ in include_pos and token.lemma_ not in stopwords)\n",
    "\n",
    "# 出現頻度top 10を出力する\n",
    "for word, count in counter.most_common(10):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f257178",
   "metadata": {},
   "source": [
    "更に、単語の共起を見てみます。共起とは、どの語とどの語が一緒に使われているかを調べる方法です。\n",
    "\n",
    "文章を文に分割し、同一文中に同時に出現する単語の組を数え上げることで分析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8988aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbe898a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent, pos_tags, stopwords):\n",
    "    \"\"\"\n",
    "    分析対象の品詞であり、不要語ではない単語を抽出する\n",
    "    \"\"\"\n",
    "    words = [token.lemma_ for token in sent if token.pos_ in pos_tags and token.lemma_ not in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54b1a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cooccurrence(sents, token_length=\"{2,}\"):\n",
    "    \"\"\"\n",
    "    同じ文中に共起する単語を行列形式で列挙する\n",
    "    \"\"\"\n",
    "    token_pattern = f\"\\\\b\\\\w{token_length}\\\\b\"\n",
    "    count_model = CountVectorizer(token_pattern=token_pattern)\n",
    "    \n",
    "    X = count_model.fit_transform(sents)\n",
    "    words = count_model.get_feature_names_out()\n",
    "    word_counts = np.asarray(X.sum(axis=0).reshape(-1))\n",
    "    \n",
    "    X[X > 0] = 1 # 同じ共起が2以上出現しても1とする\n",
    "    Xc = (X.T * X) # 共起行列を求めるための掛け算をする、csr形式の疎行列\n",
    "    \n",
    "    return words, word_counts, Xc, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04aa7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence_by_cooccurrence(X, idxs):\n",
    "    \"\"\"\n",
    "    指定された共起を含む文を見つける\n",
    "    \"\"\"\n",
    "    occur_flags = (X[:, idxs[0]] > 0)\n",
    "    for idx in idxs[1:]:\n",
    "        occur_flags = occur_flags.multiply(X[:, idx] > 0)\n",
    "    \n",
    "    return occur_flags.nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23665546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昨日の天気は晴れでした。\n",
      "明日も晴れるでしょう。\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae73d3",
   "metadata": {},
   "source": [
    "### 係り受け解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50bc9b",
   "metadata": {},
   "source": [
    "### 分類"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a763cb",
   "metadata": {},
   "source": [
    "### クラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83332c67",
   "metadata": {},
   "source": [
    "## 可視化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
